{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-V-xD9Y64hM95-I4uqC7J20KgHMjgo_d",
      "authorship_tag": "ABX9TyOElfHy4DIhCYsD/g0a1dy4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NVN404/ai-assignment/blob/main/Q3code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "671bb991",
        "outputId": "565d87d6-df71-4d22-9c59-abea93c36d0b"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/drive')\n",
        "baseDir = '/drive/MyDrive/aksharantar_sampled'\n",
        "lang = 'tam'\n",
        "trainPath = os.path.join(baseDir, lang, f'{lang}_train.csv')\n",
        "validPath = os.path.join(baseDir, lang, f'{lang}_valid.csv')\n",
        "testPath = os.path.join(baseDir, lang, f'{lang}_test.csv')\n",
        "\n",
        "if not (os.path.exists(trainPath) and os.path.exists(validPath) and os.path.exists(testPath)):\n",
        "    raise FileNotFoundError(\"One or more dataset files missing. Check baseDir and lang variables.\")\n",
        "trainDf = pd.read_csv(trainPath, header=0)\n",
        "validDf = pd.read_csv(validPath, header=0)\n",
        "testDf = pd.read_csv(testPath, header=0)\n",
        "\n",
        "# Converting dataframes to list of pairs\n",
        "trainPairs = list(zip(trainDf.iloc[:,0].astype(str).tolist(), trainDf.iloc[:,1].astype(str).tolist()))\n",
        "validPairs = list(zip(validDf.iloc[:,0].astype(str).tolist(), validDf.iloc[:,1].astype(str).tolist()))\n",
        "testPairs = list(zip(testDf.iloc[:,0].astype(str).tolist(), testDf.iloc[:,1].astype(str).tolist()))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Models hyperparameters\n",
        "embedDim = 128\n",
        "hiddenDim = 256\n",
        "encoderLayers = 1\n",
        "decoderLayers = 1\n",
        "rnnType = 'GRU'\n",
        "batchSize = 64\n",
        "learningRate = 1e-3\n",
        "epochs = 20\n",
        "teacherForcingRatio = 0.5\n",
        "modelSavePath = f'/drive/MyDrive/akshantar_models/{lang}_seq2seq_attention.pth' # Changed model name\n",
        "os.makedirs(os.path.dirname(modelSavePath), exist_ok=True)\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, chars, name):\n",
        "        self.name = name\n",
        "        self.charToIndex = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2}\n",
        "        self.indexToChar = {0:\"<pad>\", 1:\"<sos>\", 2:\"<eos>\"}\n",
        "        self.size = len(self.charToIndex)\n",
        "        for c in sorted(list(chars)):\n",
        "            self.addChar(c)\n",
        "            #<pad> (for padding short sentences), <sos> (Start-of-String), and <eos> (End-of-String)\n",
        "\n",
        "    def addChar(self, char):\n",
        "        if char not in self.charToIndex:\n",
        "            self.charToIndex[char] = self.size\n",
        "            self.indexToChar[self.size] = char\n",
        "            self.size += 1\n",
        "\n",
        "def buildVocabs(pairs):\n",
        "    srcChars = set(c for s,t in pairs for c in s)\n",
        "    tgtChars = set(c for s,t in pairs for c in t)\n",
        "    srcVocab = Vocab(srcChars, \"latin\")\n",
        "    tgtVocab = Vocab(tgtChars, \"native\")\n",
        "    return srcVocab, tgtVocab\n",
        "srcVocab, tgtVocab = buildVocabs(trainPairs)\n",
        "# creating two objects one is latin another one is native\n",
        "\n",
        "combinedPairs = trainPairs + validPairs + testPairs\n",
        "maxSrcLen = max(len(s) for s,t in combinedPairs) + 1\n",
        "maxTgtLen = max(len(t) for s,t in combinedPairs) + 2\n",
        "\n",
        "\n",
        "# this is a pyTorch class which converts this pair latin and native to numerical tensors\n",
        "class TransliterationDataset(Dataset):\n",
        "    def __init__(self, pairs, srcV, tgtV, maxSrcLen, maxTgtLen):\n",
        "        self.pairs = pairs\n",
        "        self.srcV = srcV\n",
        "        self.tgtV = tgtV\n",
        "        self.maxSrcLen = maxSrcLen\n",
        "        self.maxTgtLen = maxTgtLen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def encodeSrc(self, s):\n",
        "        inds = [self.srcV.charToIndex.get(c, self.srcV.charToIndex[\"<pad>\"]) for c in s] + [self.srcV.charToIndex[\"<eos>\"]]\n",
        "        pad = [self.srcV.charToIndex[\"<pad>\"]] * (self.maxSrcLen - len(inds))\n",
        "        inds.extend(pad)\n",
        "        return torch.tensor(inds, dtype=torch.long)\n",
        "\n",
        "    def encodeTgt(self, t):\n",
        "        inds = [self.tgtV.charToIndex[\"<sos>\"]] + [self.tgtV.charToIndex.get(c, self.tgtV.charToIndex[\"<pad>\"]) for c in t] + [self.tgtV.charToIndex[\"<eos>\"]]\n",
        "        pad = [self.tgtV.charToIndex[\"<pad>\"]] * (self.maxTgtLen - len(inds))\n",
        "        inds.extend(pad)\n",
        "        return torch.tensor(inds, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s,t = self.pairs[idx]\n",
        "        return self.encodeSrc(s).to(device), self.encodeTgt(t).to(device)\n",
        "\n",
        "trainDataset = TransliterationDataset(trainPairs, srcVocab, tgtVocab, maxSrcLen, maxTgtLen)\n",
        "validDataset = TransliterationDataset(validPairs, srcVocab, tgtVocab, maxSrcLen, maxTgtLen)\n",
        "testDataset = TransliterationDataset(testPairs, srcVocab, tgtVocab, maxSrcLen, maxTgtLen)\n",
        "trainLoader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
        "validLoader = DataLoader(validDataset, batch_size=batchSize, shuffle=False)\n",
        "testLoader = DataLoader(testDataset, batch_size=batchSize, shuffle=False)\n",
        "\n",
        "#it reads the input word and compress it into a set of memory vectors and a final summary vector.\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, inputVocabSize, embedDim, hiddenDim, numLayers, rnnType):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(inputVocabSize, embedDim, padding_idx=0)\n",
        "        if rnnType == 'GRU':\n",
        "            self.rnn = nn.GRU(embedDim, hiddenDim, numLayers, batch_first=True)\n",
        "        elif rnnType == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedDim, hiddenDim, numLayers, batch_first=True)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(embedDim, hiddenDim, numLayers, batch_first=True)\n",
        "\n",
        "    def forward(self, src):\n",
        "        emb = self.embedding(src)\n",
        "        outputs, hidden = self.rnn(emb)\n",
        "        return outputs, hidden\n",
        "\n",
        "# It scores how well the decoder's state matches each of the encoder's states.kinda like focus mechanism .\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hiddenDim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hiddenDim * 2, hiddenDim)\n",
        "        self.v = nn.Linear(hiddenDim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoderOutputs):\n",
        "        batchSize = encoderOutputs.shape[0]\n",
        "        srcLen = encoderOutputs.shape[1]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, srcLen, 1)\n",
        "        combined = torch.cat((hidden, encoderOutputs), dim=2)\n",
        "        energy = torch.tanh(self.attn(combined))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "#it generates the output word one character at a time, using the encoder's outputs and the attention\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, outputVocabSize, embedDim, hiddenDim, numLayers, rnnType, attention):\n",
        "        super().__init__()\n",
        "        self.outputVocabSize = outputVocabSize\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(outputVocabSize, embedDim, padding_idx=0)\n",
        "        if rnnType == 'GRU':\n",
        "            self.rnn = nn.GRU(embedDim + hiddenDim, hiddenDim, numLayers, batch_first=True)\n",
        "        elif rnnType == 'LSTM':\n",
        "            self.rnn = nn.LSTM(embedDim + hiddenDim, hiddenDim, numLayers, batch_first=True)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(embedDim + hiddenDim, hiddenDim, numLayers, batch_first=True)\n",
        "        self.out = nn.Linear(hiddenDim * 2, outputVocabSize)\n",
        "\n",
        "    def forward(self, inputChar, hidden, encoderOutputs):\n",
        "        embedded = self.embedding(inputChar)\n",
        "        if isinstance(self.rnn, nn.LSTM):\n",
        "             attnWeights = self.attention(hidden[0][-1,:,:], encoderOutputs)\n",
        "        else: # GRU or RNN\n",
        "             attnWeights = self.attention(hidden[-1,:,:], encoderOutputs)\n",
        "        attnWeights = attnWeights.unsqueeze(1)\n",
        "        context = torch.bmm(attnWeights, encoderOutputs)\n",
        "        rnnInput = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.rnn(rnnInput, hidden)\n",
        "        combined = torch.cat((output.squeeze(1), context.squeeze(1)), dim=1)\n",
        "        pred = self.out(combined)\n",
        "        return pred, hidden\n",
        "\n",
        "#  orchestrates the entire proces\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, targetVocabSize):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.targetVocabSize = targetVocabSize\n",
        "\n",
        "    def forward(self, src, tgt, teacherForcingRatio=0.5):\n",
        "        batchSize = src.shape[0]\n",
        "        maxTgt = tgt.shape[1]\n",
        "        outputs = torch.zeros(batchSize, maxTgt, self.targetVocabSize, device=device)\n",
        "        encoder_outputs, encHidden = self.encoder(src)\n",
        "        decoder_hidden = encHidden\n",
        "        decoder_input = tgt[:, 0].unsqueeze(1)\n",
        "\n",
        "        for t in range(1, maxTgt):\n",
        "            output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            outputs[:, t, :] = output\n",
        "            teacher_force = random.random() < teacherForcingRatio\n",
        "            top1 = output.argmax(1)\n",
        "            decoder_input = tgt[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
        "        return outputs\n",
        "\n",
        "def initWeights(m): # give the model's weights a smart random starting point for training\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "attention = Attention(hiddenDim).to(device)\n",
        "encoder = Encoder(srcVocab.size, embedDim, hiddenDim, encoderLayers, rnnType).to(device)\n",
        "decoder = Decoder(tgtVocab.size, embedDim, hiddenDim, decoderLayers, rnnType, attention).to(device)\n",
        "model = Seq2Seq(encoder, decoder, tgtVocab.size).to(device)\n",
        "model.apply(initWeights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgtVocab.charToIndex[\"<pad>\"])\n",
        "\n",
        "# Training function\n",
        "def trainEpoch(model, loader, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    totalLoss = 0\n",
        "    for src, tgt in loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt, teacherForcingRatio)\n",
        "        outputDim = output.shape[-1]\n",
        "        outputFlat = output[:,1:,:].reshape(-1, outputDim)\n",
        "        targetFlat = tgt[:,1:].reshape(-1)\n",
        "        loss = criterion(outputFlat, targetFlat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        totalLoss += loss.item()\n",
        "    return totalLoss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    totalLoss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            output = model(src, tgt, 0.0)\n",
        "            outputDim = output.shape[-1]\n",
        "            outputFlat = output[:,1:,:].reshape(-1, outputDim)\n",
        "            targetFlat = tgt[:,1:].reshape(-1)\n",
        "            loss = criterion(outputFlat, targetFlat)\n",
        "            totalLoss += loss.item()\n",
        "    return totalLoss / len(loader)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(1, epochs+1):\n",
        "    trainLoss = trainEpoch(model, trainLoader, optimizer, criterion, clip=1)\n",
        "    validLoss = evaluate(model, validLoader, criterion)\n",
        "    print(f'Epoch {epoch:02d} | Train Loss: {trainLoss:.4f} | Valid Loss: {validLoss:.4f}')\n",
        "    torch.save({'epoch':epoch,'model_state':model.state_dict(),'optimizer_state':optimizer.state_dict()}, modelSavePath)\n",
        "print(\"Training finished.\")\n",
        "\n",
        "def infer(model, srcText, srcV, tgtV, maxTgtLen):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        srcIdx = [srcV.charToIndex.get(c, srcV.charToIndex[\"<pad>\"]) for c in srcText] + [srcV.charToIndex[\"<eos>\"]]\n",
        "        pad = [srcV.charToIndex[\"<pad>\"]] * (maxSrcLen - len(srcIdx))\n",
        "        srcIdx.extend(pad)\n",
        "        srcTensor = torch.tensor(srcIdx, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        encoder_outputs, encHidden = model.encoder(srcTensor)\n",
        "        decHidden = encHidden\n",
        "        decInput = torch.tensor([[tgtV.charToIndex[\"<sos>\"]]], dtype=torch.long, device=device)\n",
        "\n",
        "        preds = []\n",
        "        for _ in range(maxTgtLen):\n",
        "            out, decHidden = model.decoder(decInput, decHidden, encoder_outputs)\n",
        "            top1 = out.argmax(1)\n",
        "            ch = tgtV.indexToChar[top1.item()]\n",
        "            if ch == \"<eos>\" or ch == \"<pad>\":\n",
        "                break\n",
        "            preds.append(ch)\n",
        "            decInput = top1.unsqueeze(1)\n",
        "        return \"\".join(preds)\n",
        "\n",
        "print(\"\\nSample predictions on test set:\")\n",
        "for i in range(10):\n",
        "    s,t = testPairs[i]\n",
        "    pred = infer(model, s, srcVocab, tgtVocab, maxTgtLen)\n",
        "    print(f'Input: {s:<12} | Target: {t:<12} | Pred: {pred:<12}')\n",
        "\n",
        "def testAccuracy(model, loader, tgtV):\n",
        "    model.eval()\n",
        "    totalTokens = 0\n",
        "    correctTokens = 0\n",
        "    ignore_indices = [tgtV.charToIndex[\"<pad>\"], tgtV.charToIndex[\"<sos>\"], tgtV.charToIndex[\"<eos>\"]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in loader:\n",
        "            output = model(src, tgt, 0.0)\n",
        "            preds = output.argmax(-1)\n",
        "            tgtFlat = tgt[:,1:]\n",
        "            predsFlat = preds[:,1:]\n",
        "            mask = torch.ones_like(tgtFlat, dtype=torch.bool, device=device)\n",
        "            for idx in ignore_indices:\n",
        "                mask &= (tgtFlat != idx)\n",
        "            totalTokens += mask.sum().item()\n",
        "            correctTokens += ((predsFlat == tgtFlat) & mask).sum().item()\n",
        "\n",
        "    return correctTokens / totalTokens if totalTokens > 0 else 0.0\n",
        "\n",
        "acc = testAccuracy(model, testLoader, tgtVocab)\n",
        "print(f'\\nTest token-level accuracy: {acc*100:.2f}%')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "Starting training...\n",
            "Epoch 01 | Train Loss: 0.9740 | Valid Loss: 0.8254\n",
            "Epoch 02 | Train Loss: 0.2172 | Valid Loss: 0.6881\n",
            "Epoch 03 | Train Loss: 0.1702 | Valid Loss: 0.7314\n",
            "Epoch 04 | Train Loss: 0.1508 | Valid Loss: 0.7534\n",
            "Epoch 05 | Train Loss: 0.1351 | Valid Loss: 0.7200\n",
            "Epoch 06 | Train Loss: 0.1249 | Valid Loss: 0.6939\n",
            "Epoch 07 | Train Loss: 0.1162 | Valid Loss: 0.6721\n",
            "Epoch 08 | Train Loss: 0.1079 | Valid Loss: 0.7118\n",
            "Epoch 09 | Train Loss: 0.1005 | Valid Loss: 0.7315\n",
            "Epoch 10 | Train Loss: 0.0921 | Valid Loss: 0.7196\n",
            "Epoch 11 | Train Loss: 0.0892 | Valid Loss: 0.7525\n",
            "Epoch 12 | Train Loss: 0.0796 | Valid Loss: 0.7300\n",
            "Epoch 13 | Train Loss: 0.0745 | Valid Loss: 0.7722\n",
            "Epoch 14 | Train Loss: 0.0739 | Valid Loss: 0.7774\n",
            "Epoch 15 | Train Loss: 0.0652 | Valid Loss: 0.8307\n",
            "Epoch 16 | Train Loss: 0.0650 | Valid Loss: 0.8020\n",
            "Epoch 17 | Train Loss: 0.0590 | Valid Loss: 0.8240\n",
            "Epoch 18 | Train Loss: 0.0507 | Valid Loss: 0.8173\n",
            "Epoch 19 | Train Loss: 0.0489 | Valid Loss: 0.8696\n",
            "Epoch 20 | Train Loss: 0.0552 | Valid Loss: 0.8481\n",
            "Training finished.\n",
            "\n",
            "Sample predictions on test set:\n",
            "Input: pirandhana   | Target: பிறந்தன      | Pred: பிறந்தன     \n",
            "Input: paantvitth   | Target: பாண்ட்விட்த் | Pred: பாண்ட்வித்  \n",
            "Input: payanpaduththavillai | Target: பயன்படுத்தவில்லை | Pred: பயன்படுத்தவில்லை\n",
            "Input: chithiraip   | Target: சித்திரைப்   | Pred: சிதிரைப்    \n",
            "Input: veetak       | Target: வேதக்        | Pred: வீடக்       \n",
            "Input: veettirkuch  | Target: வீட்டிற்குச் | Pred: வீட்டிற்குச்\n",
            "Input: seenivasa    | Target: சீனிவாச      | Pred: சீனிவச      \n",
            "Input: siridhalavu  | Target: சிறிதளவு     | Pred: சிரிதளவு    \n",
            "Input: parappazhavai | Target: பரப்பளவை     | Pred: பரப்பழவை    \n",
            "Input: kaththirukkakudaathu | Target: காத்திருக்ககூடாது | Pred: கத்திருக்ககுடாது\n",
            "\n",
            "Test token-level accuracy: 81.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "maGRzka4EfNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}